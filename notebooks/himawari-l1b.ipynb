{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efa2b6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from goes2go.himawari_data import _himawari_file_df\n",
    "from goes2go.gk2a_data import gk2a_timerange \n",
    "from satpy import Scene\n",
    "import pandas as pd\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37d45168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dates for HIMAWARI-8\n",
    "# Allowed start: 7/7/2015 02:00\n",
    "# Allowed end: 13/12/2022 04:50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "50648715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from satpy import available_readers\n",
    "available_readers()\n",
    "\n",
    "import s3fs\n",
    "\n",
    "# Connect to AWS public buckets\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "# Define parameter options and aliases\n",
    "# ------------------------------------\n",
    "_himawari_satellite = {\n",
    "    \"noaa-himawari8\": [8, \"8\", \"H8\", \"HIMAWARI8\", \"HIMAWARI-8\"], \n",
    "    \"noaa-himawari9\": [9, \"9\", \"H9\", \"HIMAWARI9\", \"HIMAWARI-9\"], \n",
    "}\n",
    "\n",
    "_himawari_domain = {\n",
    "    \"Japan\": [\"C\", \"CONUS\", \"JAPAN\"],\n",
    "    \"FLDK\": [\"F\", \"FULL\", \"FULLDISK\", \"FULL DISK\"],\n",
    "    \"Target\": [\"M\", \"MESOSCALE\", \"M1\", \"M2\", \"TARGET\"],\n",
    "}\n",
    "\n",
    "_himawari_resolution = {\n",
    "    \"R05\": [0.5, 500, \"0.5\", \"500\"], \n",
    "    \"R10\": [1, 1000, \"1\", \"1000\"], \n",
    "    \"R20\": [2, 2000, \"2\", \"2000\"], \n",
    "}\n",
    "\n",
    "_himawari_bands = dict(\n",
    "    zip(\n",
    "        [\n",
    "            \"B01\",\n",
    "            \"B02\",\n",
    "            \"B03\",\n",
    "            \"B04\",\n",
    "            \"B05\",\n",
    "            \"B06\",\n",
    "            \"B07\",\n",
    "            \"B08\",\n",
    "            \"B09\",\n",
    "            \"B10\",\n",
    "            \"B11\",\n",
    "            \"B12\",\n",
    "            \"B13\",\n",
    "            \"B14\",\n",
    "            \"B15\",\n",
    "            \"B16\",\n",
    "        ],\n",
    "        range(1, 16 + 1),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b7999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _himawari_file_df(satellite, domain, start, end, bands=None, resolutions=None, refresh=True, ignore_missing=False):\n",
    "    \"\"\"Get list of requested GOES files as pandas.DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    satellite : str\n",
    "    product : str\n",
    "    start : datetime\n",
    "    end : datetime\n",
    "    band : None, int, or list\n",
    "        Specify the ABI channels to retrieve.\n",
    "    refresh : bool\n",
    "        Refresh the s3fs.S3FileSystem object when files are listed.\n",
    "        Default True will refresh and not use a cached list.\n",
    "    \"\"\"\n",
    "    params = locals()\n",
    "\n",
    "    start = pd.to_datetime(start)\n",
    "    end = pd.to_datetime(end)\n",
    "\n",
    "    DATES = pd.date_range(f\"{start:%Y-%m-%d %H:00}\", f\"{end:%Y-%m-%d %H:00}\", freq=\"600s\")\n",
    "\n",
    "    # List all files for each date\n",
    "    # ----------------------------\n",
    "    files = []\n",
    "    for DATE in DATES:\n",
    "        path = f\"{satellite}/AHI-L1b-{domain}/{DATE:%Y/%m/%d/%H%M}\"\n",
    "        if ignore_missing is True:\n",
    "            try:\n",
    "                files += fs.ls(path, refresh=refresh)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Ignored missing dir: {path}\")\n",
    "        else:\n",
    "            files += fs.ls(path, refresh=refresh)\n",
    "\n",
    "\n",
    "    # Build a table of the files\n",
    "    # --------------------------\n",
    "    df = pd.DataFrame(files, columns=[\"file\"])\n",
    "    df.drop(index=df.index[~df[\"file\"].str.contains(\".DAT.bz2\")],inplace=True)\n",
    "    df[[\"data_format\", \"satellite\", \"date\", \"time\", \"band\", \"domain\", \"resolution\", \"sector\"]] = (\n",
    "        df[\"file\"]\n",
    "        .str.rsplit(\"/\", expand=True)\n",
    "        .iloc[:, -1]\n",
    "        .str.rsplit(\".\", expand=True)\n",
    "        .loc[:, 0]\n",
    "        .str.rsplit(\"_\", expand=True)\n",
    "    )\n",
    "\n",
    "    # Filter files by band number\n",
    "    # ---------------------------\n",
    "    if bands is not None:\n",
    "        if not hasattr(bands, \"__len__\") or isinstance(bands, (str, bytes, bytearray)):\n",
    "            bands = [bands]\n",
    "        for i_band, band in enumerate(bands):\n",
    "            if band not in _himawari_bands:\n",
    "                try:\n",
    "                    bands[i_band] = dict(zip(_himawari_bands.values(), _himawari_bands.keys()))[\n",
    "                        band\n",
    "                    ]\n",
    "                except KeyError:\n",
    "                    raise ValueError(f\"Band {band} is not a valid AHI channel\")\n",
    "        df = df.loc[df.band.isin(bands)]\n",
    "\n",
    "    # Filter files by resolution\n",
    "    # --------------------------\n",
    "    if resolutions is not None:\n",
    "        if not hasattr(resolutions, \"__len__\") or isinstance(resolutions, (str, bytes, bytearray)):\n",
    "            resolutions = [resolutions]\n",
    "        for i_resolution, resolution in enumerate(resolutions):\n",
    "            if resolution not in _himawari_resolution:\n",
    "                for key, aliases in _himawari_resolution.items():\n",
    "                    if resolution in aliases:\n",
    "                        resolutions[i_resolution] = key\n",
    "                        break\n",
    "                else:\n",
    "                    raise ValueError(f\"Resolution {resolution} is not a valid AHI resolution\")\n",
    "        df = df.loc[df.resolution.isin(resolutions)]\n",
    "    else:\n",
    "        # If None pick the highest resolution for each\n",
    "        df = df.loc[df.resolution == df.groupby(\"band\").resolution.unique().str[0][df.band].to_numpy()]\n",
    "    \n",
    "    # Filter files by requested time range\n",
    "    # ------------------------------------\n",
    "    # Convert filename datetime string to datetime object\n",
    "    df[\"time\"] = pd.to_datetime(df.date + df.time, format=\"%Y%m%d%H%M\")\n",
    "\n",
    "    # Filter by files within the requested time range\n",
    "    df = df.loc[df.time >= start].loc[df.time < end].reset_index(drop=True)\n",
    "\n",
    "    for i in params:\n",
    "        df.attrs[i] = params[i]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4300fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2022-04-01T03:15:32\"\n",
    "end_date = \"2022-04-01T04:15:32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cd8a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find himawari files\n",
    "himawari_df = _himawari_file_df(\"noaa-himawari8\", \"FLDK\", start_date, end_date, ignore_missing=True) # start_date and end_date must be end with a multiple of ten minutes, e.g. minute=0, 10, 20..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6315c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "selected = np.random.choice(himawari_df.time.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18154f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with satpy\n",
    "scn = Scene(\n",
    "    [f's3://{f}' for f in himawari_df.groupby(\"time\").get_group(selected).file], # select all files at one time\n",
    "    reader=\"ahi_hsd\", \n",
    "    reader_kwargs=dict(\n",
    "        storage_options={\n",
    "            'anon': True,\n",
    "            'default_block_size': 100*1024*1024,  # 100MB blocks for large files\n",
    "            'default_cache_type': 'readahead',     # Optimize for sequential reading\n",
    "        }\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d47e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "scn.load(scn.all_dataset_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb77320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample to 2km resolution\n",
    "new_scn = scn.resample(scn.coarsest_area(), resampler='native')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cd8e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to xarray\n",
    "himawari_ds = new_scn.to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c637ff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy.datetime64 to pandas datetime first, then format\n",
    "pd.to_datetime(selected).strftime('%Y%m%d%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c2c2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "himawari_ds.B01.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a55b733",
   "metadata": {},
   "source": [
    "### GeoKompsat 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d6f91ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_files = gk2a_timerange(\n",
    "    start=\"2023-10-01 10:00:00\",\n",
    "    end=\"2023-10-01 10:10:00\",\n",
    "    domain=\"FD\",\n",
    "    download=False\n",
    ")\n",
    "ami_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a6efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = list(\"s3://\"+ file for file in ami_files[\"file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f81624fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = list(\"s3://\"+ file for file in ami_files[\"file\"])\n",
    "storage_options = {'anon': True}\n",
    "scn = Scene(reader='ami_l1b', filenames=filenames, reader_kwargs={'storage_options': storage_options})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f32b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "scn = Scene(reader='ami_l1b', filenames=[filenames[0]], reader_kwargs={'storage_options': storage_options})\n",
    "scn.load(['IR087',\n",
    " 'IR096',\n",
    " 'IR105',\n",
    " 'IR112',\n",
    " 'IR123',\n",
    " 'IR133',\n",
    " 'NR013',\n",
    " 'NR016',\n",
    " 'SW038',\n",
    "#  'VI004',\n",
    "#  'VI005',\n",
    "#  'VI006',\n",
    "#  'VI008',\n",
    " 'WV063',\n",
    " 'WV069',\n",
    " 'WV073'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772293d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = scn.to_xarray()\n",
    "ds.to_netcdf('test.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849b3005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "test = xr.open_dataset('test.nc')\n",
    "test.IR087"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "db37b8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_file_size(ds, compression_level=9):\n",
    "    \"\"\"\n",
    "    Reduce the file size of the dataset by converting to float32 and compressing.\n",
    "    \n",
    "    Args:\n",
    "        ds (xarray.Dataset): The dataset to reduce.\n",
    "        compression_level (int): Compression level for saving the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        xarray.Dataset: Reduced dataset.\n",
    "    \"\"\"\n",
    "    # Reduce file size by converting to float32\n",
    "    ds = ds.astype(\"float32\")\n",
    "    # Remove unnecessary variables\n",
    "    ds = ds.drop_vars([\"FLDK\"])\n",
    "\n",
    "    encoding = {}\n",
    "\n",
    "    # Add data variable compression\n",
    "    for var in ds.data_vars:\n",
    "        if ds[var].dtype in ['float64', 'float32']:\n",
    "            encoding[var] = {'dtype': 'float32', 'zlib': True, 'complevel': compression_level, 'shuffle': True}\n",
    "    # Add coordinate compression\n",
    "    for coord in ds.coords:\n",
    "        if ds[coord].dtype in ['float64', 'float32']:\n",
    "            encoding[coord] = {'dtype': 'float32', 'zlib': True, 'complevel': compression_level, 'shuffle': True}\n",
    "\n",
    "    return ds, encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "fdeb1495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "test_ds = xr.open_dataset('20210128132000_patch_2150_2389.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec820821",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4023b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_ds, encoding = reduce_file_size(test_ds, compression_level=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3d46d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_ds.to_netcdf('20210128132000_patch_2150_2389_reduced.nc', encoding=encoding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esl3d-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
